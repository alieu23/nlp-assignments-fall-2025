{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "304e37a2",
   "metadata": {},
   "source": [
    "# CS4765/6765 NLP Assignment 3: Word vectors\n",
    "\n",
    "**Due 7 November at 23:59**\n",
    "\n",
    "In this three part assignment you will first examine and interact with word vectors. (This part of the assignment is adapted from a CS224N assignment at Stanford.) You will then implement a new approach to sentiment analysis. Finally you will consider possible harms of classification.\n",
    "\n",
    "In this assignment we will use [gensim](https://radimrehurek.com/gensim/) to access and interact with word embeddings. In gensim we’ll be working with a KeyedVectors object which represents word embeddings. [Documentation for KeyedVectors is available.](https://radimrehurek.com/gensim/models/keyedvectors.html) However, this assignment description and the sample code in it might be sufficient to show you how to use a KeyedVectors object. We will use word embeddings from Google that were pretrained on about 100B words from a Google News dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "0ffb28a8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T22:52:46.003141Z",
     "start_time": "2025-11-07T22:51:58.543627Z"
    }
   },
   "source": [
    "import gensim.downloader\n",
    "model = gensim.downloader.load('word2vec-google-news-300')"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "192b65d9",
   "metadata": {},
   "source": [
    "# Part 1: Examining word vectors (6 marks)\n",
    "\n",
    "## Polysemy and homonymy\n",
    "\n",
    "Polysemy and homonymy are the phenomena of words having multiple meanings/senses. The nearest neighbours (under cosine similarity) for a given word can indicate whether it has multiple senses.\n",
    "\n",
    "Consider the following example which shows the top-10 most similar words for *mouse*. The \"input device\" and \"animal\" senses of *mouse* are clearly visible from the top-10 most similar words. \n"
   ]
  },
  {
   "cell_type": "code",
   "id": "a32120e3",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2025-11-07T22:57:23.531830Z",
     "start_time": "2025-11-07T22:54:43.534286Z"
    }
   },
   "source": [
    "# Find words most similar using cosine similarity to \"mouse\". \n",
    "# restrict_vocab=100000 limits the results to most frequent\n",
    "# 100000 words. This avoids rare words in the output. For this\n",
    "# assignment, whenever you call most_simlilar, also pass\n",
    "# restrict_vocab=100000.\n",
    "model.most_similar('mouse', restrict_vocab=100000)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('mice', 0.5896884799003601),\n",
       " ('cursor', 0.5472042560577393),\n",
       " ('joystick', 0.525871753692627),\n",
       " ('Wiimote', 0.502321720123291),\n",
       " ('Mouse', 0.4949929118156433),\n",
       " ('stylus', 0.4937674403190613),\n",
       " ('hamster', 0.4880635142326355),\n",
       " ('keyboard', 0.4737585783004761),\n",
       " ('hamsters', 0.4688059687614441),\n",
       " ('trackpad', 0.4672960937023163)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "2f34e2ac",
   "metadata": {},
   "source": [
    "*cursor*, *joystick*, *Wiimote*, *stylus*, *keyboard*, and *trackpad* correspond to the input device sense. *hamster* and *hamsters* correspond to the animal sense. (You can observe something similar for the different senses of the word *leaves*.)\n",
    "\n",
    "Find a new example that exhibits polysemy/homonymy, show its top-10 most similar words, and explain why they show that this word has multiple senses. Write your answer in the code and text boxes below."
   ]
  },
  {
   "cell_type": "code",
   "id": "f0194298",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T23:05:02.186008Z",
     "start_time": "2025-11-07T23:05:02.158864Z"
    }
   },
   "source": [
    "# TODO Write your code here\n",
    "model.most_similar('cloud', restrict_vocab=100000)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('clouds', 0.7632127404212952),\n",
       " ('Cloud', 0.6046801805496216),\n",
       " ('cloud_computing', 0.5641706585884094),\n",
       " ('dark_clouds', 0.5420233011245728),\n",
       " ('Cloud_computing', 0.5265730023384094),\n",
       " ('Clouds', 0.5223086476325989),\n",
       " ('pall', 0.5148372054100037),\n",
       " ('SaaS', 0.499646931886673),\n",
       " ('Amazon_EC2', 0.4842623770236969),\n",
       " ('virtualized', 0.4706108272075653)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "id": "86905a9c",
   "metadata": {},
   "source": [
    "This word cloud appears to have multiple senses split between terms related to weather and terms related to computing\n",
    "### 1. Weather Terms\n",
    "    ('clouds', 0.7632127404212952),\n",
    "    ('Cloud', 0.6046801805496216),\n",
    "    ('dark_clouds', 0.5420233011245728),\n",
    "\n",
    "\n",
    "### 2. Computing Term\n",
    "    ('Cloud_computing', 0.5265730023384094),\n",
    "    ('Clouds', 0.5223086476325989),\n",
    "    ('SaaS', 0.499646931886673),\n",
    "    ('Amazon_EC2', 0.4842623770236969),\n",
    "    ('virtualized', 0.4706108272075653)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e90ead9",
   "metadata": {},
   "source": [
    "## Synonyms and antonyms\n",
    "\n",
    "The words *fast* and *speedy* are (near) synonyms (i.e., they have roughly the same meaning) and the words *fast* and *slow* are antonyms (i.e., they have opposite meanings). Note, however, that the similarity between *fast* and *slow* is higher than the similarity between *fast* and *speedy*. This should be counter to your expectations, because synonyms (which mean roughly the same thing) would be expected to be more similar than antonyms (which have opposite meanings)."
   ]
  },
  {
   "cell_type": "code",
   "id": "45f158c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T13:50:46.318893Z",
     "start_time": "2025-11-07T13:50:46.294341Z"
    }
   },
   "source": [
    "# Find the cosine similarity between \"fast\" and \"speedy\"\n",
    "model.similarity('fast', 'speedy')"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float32(0.47171298)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "b5fd9873",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T13:50:49.671710Z",
     "start_time": "2025-11-07T13:50:49.656620Z"
    }
   },
   "source": [
    "# and between \"fast\" and \"slow\".\n",
    "model.similarity('fast', 'slow')"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float32(0.5313692)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "id": "40b4384c-c0d3-4612-94dc-548b9de4412b",
   "metadata": {},
   "source": [
    "Find another example that shows this. I.e., find three words (w1 , w2 , w3) such that w1 and w2 are (near) synonyms (i.e., have roughly the same meaning), and w1 and w3 are antonyms (i.e., have opposite meanings), but the similarity between w1 and w3 is higher than the similarity between w1 and w2. Explain why you think this unexpected situation might have occurred."
   ]
  },
  {
   "cell_type": "code",
   "id": "ddcfdeb6-aca0-47ee-9341-7669e3287863",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T23:10:25.820346Z",
     "start_time": "2025-11-07T23:10:25.805445Z"
    }
   },
   "source": [
    "# TODO Write your code here\n",
    "model.similarity('big', 'fat')\n"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float32(0.2534442)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T23:10:32.658985Z",
     "start_time": "2025-11-07T23:10:32.645812Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "model.similarity('big', 'small')"
   ],
   "id": "c4fdd3a2eb2749d0",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float32(0.49586785)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "id": "05c15629",
   "metadata": {},
   "source": [
    "TODO Write your answer here\n",
    "\n",
    "The antonym pair ('big', 'small') have a higher vector or most similar than the synonym pair ('big', 'fat') this due to the fact that 'big' and 'small' tends to appear more in the same context in the corpus than 'big' and 'fat'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb72540c",
   "metadata": {},
   "source": [
    "## Analogies\n",
    "\n",
    "Analogies such as *man* is to *king* as *woman* is to *X* can be solved using word embeddings. This analogy can be expressed as *X* = *woman* + *king* − *man*. The following code snippet shows how to solve this analogy with gensim. Notice that the model gets it correct! I.e., *queen* is the most similar word."
   ]
  },
  {
   "cell_type": "code",
   "id": "24c757c6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T13:51:06.276638Z",
     "start_time": "2025-11-07T13:51:06.222185Z"
    }
   },
   "source": [
    "# Find the model's predictions for the solution to the analogy\n",
    "# \"man\" is to \"king\" as \"woman\" is to X\n",
    "model.most_similar(positive=['woman', 'king'],\n",
    "                   negative=['man'],\n",
    "                   restrict_vocab=100000)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.7118193507194519),\n",
       " ('monarch', 0.6189674139022827),\n",
       " ('princess', 0.5902431011199951),\n",
       " ('crown_prince', 0.5499460697174072),\n",
       " ('prince', 0.5377321839332581),\n",
       " ('kings', 0.5236844420433044),\n",
       " ('queens', 0.518113374710083),\n",
       " ('sultan', 0.5098593235015869),\n",
       " ('monarchy', 0.5087411403656006),\n",
       " ('royal_palace', 0.5087166428565979)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "id": "0416e879",
   "metadata": {},
   "source": [
    "Find a new analogy that the model is able to answer correctly (i.e., the most-similar word is the solution to the analogy). Explain briefly why the analogy holds. For the above example, this explanation would be something along the lines of a king is a ruler who is a man and a queen is a ruler who is a woman.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "f75da5f5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T23:58:00.946918Z",
     "start_time": "2025-11-07T23:58:00.910153Z"
    }
   },
   "source": [
    "# TODO Write your code here\n",
    "model.most_similar(positive=['apple', 'vegetable'],\n",
    "                   negative=['fruit'],\n",
    "                   restrict_vocab=100000)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('potato', 0.5865278244018555),\n",
       " ('onion', 0.5616337656974792),\n",
       " ('veggie', 0.5172390937805176),\n",
       " ('tomato', 0.4943786561489105),\n",
       " ('sweet_potato', 0.49368950724601746),\n",
       " ('Vegetable', 0.4932582974433899),\n",
       " ('cauliflower', 0.482282817363739),\n",
       " ('edible', 0.4821529984474182),\n",
       " ('pumpkin', 0.4816454350948334),\n",
       " ('sunflower', 0.4751611351966858)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "id": "cdcdf5fa",
   "metadata": {},
   "source": "Here, words like potato, onion appear to be the most similar word, this likely happens because in the training corpus, 'apple' and 'fruit' appears in similar context and vegetable and potato, onion appear in similar context. Thus the difference vector between apple and fruit is similar to vegetable and potato."
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "9a49d3d19d751d2e"
  },
  {
   "cell_type": "markdown",
   "id": "e5dae136",
   "metadata": {},
   "source": [
    "## Bias\n",
    "\n",
    "Consider the examples below. The first shows the words that are most similar to *man* and *worker* and least similar to *woman*. The second shows the words that are most similar to *woman* and *worker* and least similar to *man*."
   ]
  },
  {
   "cell_type": "code",
   "id": "79b1ccfd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T13:51:15.663794Z",
     "start_time": "2025-11-07T13:51:15.636339Z"
    }
   },
   "source": [
    "# Find the words that are most similar to \"man\" and \"worker\" and\n",
    "# least similar to \"woman\".\n",
    "model.most_similar(positive=['man', 'worker'],\n",
    "                   negative=['woman'],\n",
    "                   restrict_vocab=100000)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('workers', 0.5590359568595886),\n",
       " ('laborer', 0.54481041431427),\n",
       " ('foreman', 0.5192232131958008),\n",
       " ('Worker', 0.5161596536636353),\n",
       " ('employee', 0.5094279646873474),\n",
       " ('electrician', 0.49481216073036194),\n",
       " ('janitor', 0.48718902468681335),\n",
       " ('bricklayer', 0.48253133893013),\n",
       " ('carpenter', 0.47499001026153564),\n",
       " ('workman', 0.46425172686576843)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "id": "cd781f83",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T13:51:19.473383Z",
     "start_time": "2025-11-07T13:51:19.448604Z"
    }
   },
   "source": [
    "# Find the words that are most similar to \"woman\" and \"worker\" and\n",
    "# least similar to \"man\".\n",
    "model.most_similar(positive=['woman', 'worker'],\n",
    "                   negative=['man'],\n",
    "                   restrict_vocab=100000)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('workers', 0.6582455039024353),\n",
       " ('employee', 0.5805293917655945),\n",
       " ('nurse', 0.5249921679496765),\n",
       " ('receptionist', 0.5142489671707153),\n",
       " ('migrant_worker', 0.5001609325408936),\n",
       " ('Worker', 0.4979270100593567),\n",
       " ('housewife', 0.48609837889671326),\n",
       " ('registered_nurse', 0.4846191108226776),\n",
       " ('laborer', 0.48437267541885376),\n",
       " ('coworker', 0.48212409019470215)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "id": "22a4f4e6",
   "metadata": {},
   "source": [
    "The output shows that *man* is associated with some stereotypically male jobs (e.g., *electrician*, *janitor*, *bricklayer*, *carpenter*) while *woman* is associated with some stereotypically female jobs (e.g., *nurse*, *receptionist*, *housewife*, *registered_\n",
    "nurse*). This indicates that there is gender bias in the word embeddings.\n",
    "\n",
    "Find a new example, using the same approach as above, that indicates that there is bias in the word embeddings. Briefly explain how the model output indicates that there is bias in the word embeddings. (You are by no means restricted to considering gender bias here. You are encouraged to explore other ways that embeddings might indicate bias.)"
   ]
  },
  {
   "cell_type": "code",
   "id": "8765625f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-08T00:09:53.608249Z",
     "start_time": "2025-11-08T00:09:53.589327Z"
    }
   },
   "source": [
    "\n",
    "model.most_similar(positive=['Afghanistan', 'country'],\n",
    "                   negative=['Canada'],\n",
    "                   restrict_vocab=100000)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Iraq', 0.5811731219291687),\n",
       " ('strife_torn', 0.500225305557251),\n",
       " ('insurgency', 0.4947287142276764),\n",
       " ('troops', 0.48856207728385925),\n",
       " ('Taliban_insurgency', 0.4836787283420563),\n",
       " ('Afghan', 0.4569079577922821),\n",
       " ('tribal_regions', 0.4540356397628784),\n",
       " ('counterinsurgency', 0.4527475833892822),\n",
       " ('Somalia', 0.45025157928466797),\n",
       " ('Afghans', 0.4482972323894501)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-08T00:10:59.623396Z",
     "start_time": "2025-11-08T00:10:59.601452Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# TODO Write your code here\n",
    "model.most_similar(positive=['Canada', 'country'], negative=['Afghanistan'],\n",
    "\n",
    "                   restrict_vocab=100000)"
   ],
   "id": "21abff9c46f7bf1a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('world', 0.5181863903999329),\n",
       " ('United_States', 0.5135688781738281),\n",
       " ('nation', 0.509271502494812),\n",
       " ('Unites_States', 0.48146817088127136),\n",
       " ('Canadian', 0.47444069385528564),\n",
       " ('Nova_Scotia', 0.4674588739871979),\n",
       " ('America', 0.45913150906562805),\n",
       " ('continent', 0.45745521783828735),\n",
       " ('Europe', 0.4195738434791565),\n",
       " ('Québec', 0.41737625002861023)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 29
  },
  {
   "cell_type": "markdown",
   "id": "e0ec7446",
   "metadata": {},
   "source": "shows that Word2Vec encodes some bias, as Afghanistan is associated with words like insurgency, tribal_regions, troops, while Canada is associated with world, United_State, and other regions. These associations arise because the training corpus frames Afghanistan as a conflict-heavy region.\n"
  },
  {
   "cell_type": "markdown",
   "id": "96583e72",
   "metadata": {},
   "source": [
    "# Part 2: Sentiment Analysis (2 marks)\n",
    "\n",
    "## Background and data\n",
    "\n",
    "In this part of the assignment you will revisit sentiment analysis from assignment\n",
    "2. You will need the data provided for that\n",
    "assignment.\n",
    "\n",
    "\n",
    "## Approach\n",
    "\n",
    "We will consider sentiment analysis using an average of word embeddings document representation and a multinomial logistic regression classifier. We will compare this approach to the approach using a bag-of-words document representation and logistic regression from assignment 2.\n",
    "\n",
    "Complete the function `vec_for_doc` below. (You should not modify other parts of the\n",
    "code.) This function takes a list consisting of the tokens in a document $d$. It then returns a vector $\\vec{v}$ representing the document as the average of the embeddings for the words in the document as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "d = w_1, w_2, ... w_n\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "\\vec{v} = \\dfrac{\\vec{w_1} + \\vec{w_2} + ... + \\vec{w_n}}{n}\\\\\n",
    "\\end{equation}\n",
    "\n",
    "If a word in a document does not occur in the word embedding model, you can simply ignore it. As such, $n$ above is the number of token instances in document $d$ that also occur in the embedding model (as oposed to simply the number of token instances in document $d$). (Note that we would normally need to deal with the case of a document that consists entirely of words that don't occur in the embedding model, but for this dataset and embedding model, that situation does not occur, and so for now we don't worry about it.)"
   ]
  },
  {
   "cell_type": "code",
   "id": "855e4ef9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T21:57:25.224453Z",
     "start_time": "2025-11-07T21:57:25.217466Z"
    }
   },
   "source": [
    "import math\n",
    "def normalize(v):\n",
    "    return v/math.sqrt(sum(v**2))\n",
    "\n",
    "# TODO: Implement this function. tokenized_doc is a list of tokens in\n",
    "# a document. Return a vector representation of the document as\n",
    "# described above.\n",
    "# Hints: \n",
    "# -You can get the vector for a word w using model[w] or\n",
    "#  model.get_vector(w)\n",
    "# -You can add vectors using + and sum, e.g.,\n",
    "#  model['cat'] + model['dog']\n",
    "#  sum([model['cat'], model['dog']])\n",
    "# -You can see the shape of a vector using model['cat'].shape\n",
    "# -The vector you return should have the same shape as a word vector \n",
    "# -This should be a very short function. If you're writing lots of\n",
    "#  code, you are likely off track.\n",
    "def vec_for_doc(tokenized_doc):\n",
    "\n",
    "    # TODO: Add your code here\n",
    "    valid_tokens = [w for w in tokenized_doc if w in model.key_to_index]\n",
    "\n",
    "\n",
    "    vec = sum([model[w] for w in valid_tokens]) / len(valid_tokens)\n",
    "    return vec\n"
   ],
   "outputs": [],
   "execution_count": 24
  },
  {
   "cell_type": "markdown",
   "id": "eb48b4c8-3959-4ce0-90e3-17441e547537",
   "metadata": {},
   "source": [
    "Once you've completed `vec_for_doc` above, run the code below to train logistic regresion on the training data and evaluate on the dev data"
   ]
  },
  {
   "cell_type": "code",
   "id": "3b69d0a7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T21:57:36.404164Z",
     "start_time": "2025-11-07T21:57:31.319280Z"
    }
   },
   "source": [
    "# This code is taken from the assignment 2 starter code, except where noted below\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "import re\n",
    "import csv\n",
    "\n",
    "def get_texts_and_labels(fname):\n",
    "    # Added argument for encoding\n",
    "    csv_reader = csv.reader(open(fname, encoding='utf8'))\n",
    "    # Ignore header row\n",
    "    next(csv_reader)\n",
    "    texts = []\n",
    "    labels = []\n",
    "    for line in csv_reader:\n",
    "        _,text,label = line\n",
    "        label = int(label)\n",
    "        texts.append(text)\n",
    "        labels.append(label)\n",
    "    return texts,labels\n",
    "\n",
    "word_tokenize_pattern = re.compile(r\"(?u)\\b\\w\\w+\\b\")\n",
    "def word_tokenize(s, apply_case_folding=True):\n",
    "    return [x.lower() for x in word_tokenize_pattern.findall(s)]\n",
    "\n",
    "def print_results(gold_labels, predicted_labels):\n",
    "    p,r,f,_ = precision_recall_fscore_support(gold_labels, \n",
    "                                              predicted_labels, \n",
    "                                              average='macro', \n",
    "                                              zero_division=0)\n",
    "    acc = accuracy_score(gold_labels, predicted_labels)\n",
    "\n",
    "    print(\"Precision: \", p)\n",
    "    print(\"Recall: \", r)\n",
    "    print(\"F1: \", f)\n",
    "    print(\"Accuracy: \", acc)\n",
    "    print()\n",
    "\n",
    "train_texts, train_labels = get_texts_and_labels('data/train-sample.csv')\n",
    "dev_texts, dev_labels = get_texts_and_labels('data/dev.csv')\n",
    "\n",
    "# train_vecs and dev_vecs are lists; each element is a vector\n",
    "# representing a (train or dev) document\n",
    "# (These two lines are new, i.e., not from the assignment 2 starter code)\n",
    "train_vecs = [vec_for_doc(word_tokenize(x)) for x in train_texts]\n",
    "dev_vecs = [vec_for_doc(word_tokenize(x)) for x in dev_texts]\n",
    "\n",
    "# Train logistic regression, same as A2\n",
    "lr = LogisticRegression(max_iter=500,\n",
    "                        random_state=0)\n",
    "clf = lr.fit(train_vecs, train_labels)\n",
    "dev_predictions = clf.predict(dev_vecs)\n",
    "\n",
    "print_results(dev_labels, dev_predictions)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:  0.7867410845859122\n",
      "Recall:  0.7844246902741197\n",
      "F1:  0.7854856706600893\n",
      "Accuracy:  0.785\n",
      "\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "cell_type": "markdown",
   "id": "ea03f587-c034-456c-86e7-5aa2c3b394a8",
   "metadata": {},
   "source": [
    "Finally, evaluate on the test data"
   ]
  },
  {
   "cell_type": "code",
   "id": "c13688df-a4cd-4162-965d-e350146a830d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T21:57:47.067590Z",
     "start_time": "2025-11-07T21:57:46.667293Z"
    }
   },
   "source": [
    "test_texts,test_labels = get_texts_and_labels('data/test.csv')\n",
    "test_vecs = [vec_for_doc(word_tokenize(x)) for x in test_texts]\n",
    "test_predictions = clf.predict(test_vecs)\n",
    "print_results(test_labels, test_predictions)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:  0.7230555881089105\n",
      "Recall:  0.7271777098384208\n",
      "F1:  0.7250353885574011\n",
      "Accuracy:  0.7625\n",
      "\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "cell_type": "markdown",
   "id": "3155dc3d-bcee-4b77-82ee-6239bff77b7d",
   "metadata": {},
   "source": [
    "Run the code below to replicate the test results for logistic regression from A2. (This is just for convenience so we have the numbers here. We could go look them up from A2...)"
   ]
  },
  {
   "cell_type": "code",
   "id": "bf4b3a57-9d02-4877-85b9-7967c221925a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T21:57:51.077392Z",
     "start_time": "2025-11-07T21:57:50.641897Z"
    }
   },
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vectorizer = CountVectorizer(analyzer=word_tokenize)\n",
    "train_counts = count_vectorizer.fit_transform(train_texts)\n",
    "dev_counts = count_vectorizer.transform(dev_texts)\n",
    "test_counts = count_vectorizer.transform(test_texts)\n",
    "\n",
    "lr_A2 = LogisticRegression(max_iter=500,\n",
    "                        random_state=0)\n",
    "\n",
    "clf_A2 = lr_A2.fit(train_counts, train_labels)\n",
    "\n",
    "A2_dev_predictions = clf_A2.predict(dev_counts)\n",
    "print_results(dev_labels, A2_dev_predictions)\n",
    "\n",
    "A2_test_predictions = clf_A2.predict(test_counts)\n",
    "print_results(test_labels, A2_test_predictions)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:  0.7900114973711068\n",
      "Recall:  0.7734038142620232\n",
      "F1:  0.7801965694519533\n",
      "Accuracy:  0.78\n",
      "\n",
      "Precision:  0.6405949791868577\n",
      "Recall:  0.6391179383355766\n",
      "F1:  0.6397733310398869\n",
      "Accuracy:  0.684375\n",
      "\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "cell_type": "markdown",
   "id": "543028a5-2505-4b0b-b15c-3eac6a431c40",
   "metadata": {},
   "source": [
    "Compare the results on the test data here to the results using logistic regression on the test data for assignment 2. The difference between these two approaches is the document representation. In this assignment we used a document representation based on average of word embeddings. In assignment 2 we used a document representation based on word counts. Which method performs better?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a2961a-ae6a-4232-a908-dbea1854b73e",
   "metadata": {},
   "source": "Word embeddings generally outperform bag-of-words approaches because they capture **semantic meaning and word relationships**, while word counts only track frequency. This allows embedding-based models to **generalize better**, handle **synonyms**, and understand **context**, leading to higher classification accuracy. Logistic regression performance can drop on smaller datasets, a limitation that affects word-count models more than embedding-based models.\n"
  },
  {
   "cell_type": "markdown",
   "id": "082b95bf-603d-47a4-9f56-5878e51a7ec3",
   "metadata": {},
   "source": [
    "# Part 3 Considering harms of classification (2 marks)\n",
    "\n",
    "In this part we will consider the potential for harms from the classifiers that we have built.\n",
    "\n",
    "Consider the following two sentences, which we will treat as short documents:\n",
    "\n",
    "* Betsy is discouraged by climate reports.\n",
    "* Jasmine is discouraged by climate reports.\n",
    "\n",
    "These are made up sentences inspired by Kiritchenko and Mohammad (2018), discussed in lecture 8, slide 29. Note that these sentences differ only in the first word, which is a name, and that from Kiritchenko and Mohammad, \"Betsy\" is a common European American female name and \"Jasmine\" is a common African American female name. The sentences are intended to be like those from the assignment 2 data in that they are climate related.\n",
    "\n",
    "Run the code cells below to get class predictions for these documents from the classifiers considered in Part 2.\n",
    "\n",
    "Svetlana Kiritchenko and Saif Mohammad. 2018. [Examining Gender and Race Bias in Two Hundred Sentiment Analysis Systems](https://aclanthology.org/S18-2005/). In Proceedings of the Seventh Joint Conference on Lexical and Computational Semantics, pages 43–53, New Orleans, Louisiana. Association for Computational Linguistics.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "41a51249-cdfe-4084-8dbe-33524c86edfb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T22:13:03.979456Z",
     "start_time": "2025-11-07T22:13:03.964690Z"
    }
   },
   "source": [
    "# Get the prediction for a single document using a document representation based on average of word embeddings \n",
    "def get_prediction(doc):\n",
    "    vec = vec_for_doc(word_tokenize(doc))\n",
    "    prediction = clf.predict([vec])\n",
    "    return prediction[0]\n",
    "\n",
    "# Get the prediction for a single document using a document representation based on word counts\n",
    "# (I.e., the logistic regression approach from assignment 2)\n",
    "def get_prediction_a2(doc):\n",
    "    counts = count_vectorizer.transform([doc])\n",
    "    prediction = clf_A2.predict(counts)\n",
    "    return prediction[0]"
   ],
   "outputs": [],
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "id": "02cac9c5-135b-426d-afed-5963b98cd219",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T22:13:13.204107Z",
     "start_time": "2025-11-07T22:13:13.183346Z"
    }
   },
   "source": [
    "d_betsy = \"Betsy is discouraged by climate reports.\"\n",
    "print(d_betsy)\n",
    "print(\"Word embeddings:\", get_prediction(d_betsy))\n",
    "print(\"Word counts (A2):\", get_prediction_a2(d_betsy))\n",
    "print()\n",
    "\n",
    "d_jasmine = \"Jasmine is discouraged by climate reports.\"\n",
    "print(d_jasmine)\n",
    "print(\"Word embeddings:\", get_prediction(d_jasmine))\n",
    "print(\"Word counts (A2):\", get_prediction_a2(d_jasmine))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Betsy is discouraged by climate reports.\n",
      "Word embeddings: 1\n",
      "Word counts (A2): 1\n",
      "\n",
      "Jasmine is discouraged by climate reports.\n",
      "Word embeddings: 0\n",
      "Word counts (A2): 1\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "cell_type": "markdown",
   "id": "9517e955-398c-48df-996f-cff3b42ed476",
   "metadata": {},
   "source": [
    "Based on the behaviour you observe above, write a brief discussion below about potential harms of classification from these classifiers. In your discussion, you might consider some of the following points:\n",
    "* Do the classifiers make the same predictions?\n",
    "* Is a given classifier consistent in its predictions for the two sentences?\n",
    "* If you observe differences in predictions, what do you believe causes this?\n",
    "* What do you think the gold-standard class for these sentences should be?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81539fb5-d6e7-47d5-aae3-e351f919afd9",
   "metadata": {},
   "source": "Logistic regression produces consistent predictions for both sentences, while the word embedding model gives different results. This inconsistency likely arises from biases in the pretrained embeddings, which associate names like “Jasmine” with different sentiments, even though both sentences have identical meaning and should receive the same label."
  },
  {
   "cell_type": "markdown",
   "id": "fa2f4984",
   "metadata": {},
   "source": [
    "# Submitting your work\n",
    "\n",
    "When you're done, submit a3.ipynb to the assignment 3 folder on D2L."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
